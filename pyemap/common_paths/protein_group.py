import numpy as np
from io import StringIO
import os
from ..process_data import process
from collections import OrderedDict
import networkx as nx
from networkx.algorithms import isomorphism
import time
import datetime
from ..data import char_to_res_name,res_name_to_char
import re
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.Align.Applications import MuscleCommandline
from Bio.SeqRecord import SeqRecord
from Bio.SeqUtils import seq1
#from Bio.SVDSuperimposer import SVDSuperimposer
from Bio.PDB import Superimposer
from Bio import AlignIO
from pandas import DataFrame
from numpy import linalg as LA
import string
import matplotlib.pyplot as plt


def strip_res_number(u):
    for i in range(0, len(u)):
        if u[i].isdigit():
            return u[:i]

def strip_res_num_and_chain(u):
    for i in range(0, len(u)):
        if u[i]=="_":
            u = u[:i]
            break
    start_idx = -1
    end_idx = -1
    for i in range(0, len(u)):
        if start_idx==-1 and u[i].isdigit():
            start_idx=i
        if end_idx==-1 and u[i]==")":
            end_idx = i
    return u[:start_idx] + u[end_idx+1:]

def node_match(node1, node2):
    return node1['num_label'] == node2['num_label']


def edge_match(edge1, edge2):
    return edge1['num_label'] == edge2['num_label']


class FrequentSubgraph():
    '''
    Stores all information regarding a frequent subgraph identified by the gSpan algorithm.

    Attributes
    ----------
    id: str
        Unique identifier for frequent subgraph. 
    generic_subgraph: :class:`networkx.Graph`
        Graph representation of frequent subgraph found by gSpan algorithm
    support: list of str
        List of PDB IDs which contain this subgraph
    specific_subgraphs: dict of specific subgraph id (str): :class:`networkx.Graph`
        Dict which contains specific instances of this frequent subgraph. Each entry has a unique identifier and a
        :class:`networkx.Graph` derived from the graphs generated by the :class:`~pyemap.emap` class which match 
        the pattern of this frequent subgraph.
    support_number: int
        Number of PDBs this frequent subgraph was identified in 
    '''

    def __init__(self, G, graph_number, support):
        '''Initializes FrequentSubgraph object.

        Parameters
        ----------
        G: :class:`networkx.Graph`
            Graph representation of frequent subgraph found by gSpan algorithm
        graph_number: int
            Unique numerical ID of frequent subgraph
        support: list of str
            List of PDB IDs which contain this subgraph
        '''
        self.generic_subgraph = G
        self.support = support
        self.specific_subgraphs = {}
        self.support_number = len(support)
        self.id = str(graph_number) + "_" + str(self._gen_node_rep()) + "_" + str(self.support_number)

    def general_report(self):
        ''' Generates general report which describes this frequent subgraph.
        '''
        full_str = ""
        full_str+= "ID:" + str(self.id) + "\n"
        full_str+= "Support:" + str(self.support_number) + "\n"
        full_str+= "Where:" + str(self.support) + "\n"
        full_str+= "Adjacency list:\n"
        G = self.generic_subgraph
        for node in G.nodes:
            full_str+= G.nodes[node]['label'] + str(node) + ":["
            for neighbor in G.neighbors(node):
                full_str+= G.nodes[neighbor]['label'] + str(neighbor) + "(" + str(G.edges[(node,neighbor)]['num_label']) + "), "
            full_str=full_str[:-2]
            full_str+="]\n"
        return full_str

    def full_report(self):
        full_str = self.general_report()
        full_str += str(len(self.specific_subgraphs)) + " subgraphs matching this pattern were found.\n"
        full_str += "Graphs are classified using the graph spectral method.\n\n"
        for key in self.eigenvector_sorted:
            graphs = self.eigenvector_sorted[key]
            full_str+=  "Group " + str(key) + ": " + str(len(graphs))+ " members\n---------------\n"
            for graph in graphs:
                full_str+= self._report_for_graph(graph)+ "\n"
        return full_str

    def _report_for_graph(self,G):
        full_str = ""
        full_str+= str(G.graph['id']) + "\n"
        full_str+="Nodes\n"
        for node in G.nodes:
            full_str+=G.nodes[node]['label']
            full_str+= " Position in alignment:" + str(G.nodes[node]['aligned_resnum'])
            full_str+="\n"
        full_str+="Adjacency list:\n"
        for node in G.nodes:
            full_str+= G.nodes[node]['label'] + ":["
            for neighbor in G.neighbors(node):
                dist = '{0:.2f}'.format(G.edges[(node,neighbor)]['distance'])
                full_str+= G.nodes[neighbor]['label'] + "(" + str(dist) + "), "
            full_str=full_str[:-2]
            full_str+="]\n"
        return full_str

    def _gen_node_rep(self):
        node_rep = ""
        for node, node_data in self.generic_subgraph.nodes(data=True):
            node_rep = ''.join([node_rep, node_data['label']])
        return node_rep


    def visualize_subgraph_in_ngl(self,emap,G):
        ''' Gets visualization of subgraph in ngl viewer

        Parameters
        ----------
            emap: :class:`~pyemap.emap`
                :class:`~pyemap.emap` object containing the specific subgraph
            idx: int
                Index of specific subgraph to be visualized
        '''
        colors = {"F": "orange", "Y": "blue", "W": "red", "H": "green"}
        label_texts = []
        labeled_atoms = []
        color_list = []
        selection_strs = []
        for res in G.nodes:
            label_texts.append(res)
            try:
                if res not in emap.eta_moieties:
                    color_list.append(colors[res[0]])
                    labeled_atoms.append(".CA")
                else:
                    color_list.append("pink")
                    labeled_atoms.append(next(emap.residues[res].get_atoms()).name)
            except KeyError:
                color_list.append("pink")
                labeled_atoms.append(next(emap.residues[res].get_atoms()).name)
            selection_strs.append(emap.residues[res].ngl_string)
        return label_texts, labeled_atoms, color_list, selection_strs
    
    # clustering based on graph spectral method
    def clustering(self,all_graphs):
        self.eigenvector_sorted = {}
        dims = (len(all_graphs),len(all_graphs))
        D = np.zeros(dims)
        A = np.zeros(dims)
        # adjacency matrix is distance between residues numbers in alignment
        # degree matrix is sum of all outgoing edges
        # J. Mol. Biol. (1999) 292, 441-464
        for i in range(0,len(all_graphs)):
            for j in range(i+1,len(all_graphs)):
                G1 = all_graphs[i]
                G2 = all_graphs[j]
                distance = 0
                mismatches = 0
                for k,node1 in enumerate(G1.nodes):
                    # only count for standard amino acid residues
                    if strip_res_number(node1) in char_to_res_name:
                        node2 = list(G2.nodes())[k]
                        distance+= np.absolute(G1.nodes[node1]['aligned_resnum'] - G2.nodes[node2]['aligned_resnum'])
                if distance<10:
                    A[i][j] = 1/(distance+1)
                    A[j][i] = 1/(distance+1)
                else:
                    A[i][j] = 0.01
                    A[j][i] = 0.01   
            D[i][i] = np.sum(A[i])
        # laplacian
        L = D - A
        eigv,eigvc=LA.eig(L)
        eigv = np.real(eigv)
        eigvc = np.real(eigvc)
        idx = eigv.argsort()
        eigv = eigv[idx]
        eigvc = eigvc[:,idx] 
        # second lowest eigenvector
        eigvc2 = eigvc[:,1]
        eigenvector_sorted = {}
        for i,val in enumerate(eigvc2):
            rounded_val = np.round(val,decimals=4)
            if rounded_val not in eigenvector_sorted:
                eigenvector_sorted[rounded_val] = [all_graphs[i]]
            else:
                graphs = eigenvector_sorted[rounded_val]
                graphs.append(all_graphs[i])
                eigenvector_sorted[rounded_val] =  graphs
        tuples = []
        for key,val in eigenvector_sorted.items():
            tuples.append((key,val))
        # largest groups first
        tuples.sort(key=lambda x: len(x[1]), reverse=True)
        for idx,tuple1 in enumerate(tuples):
            key,group = tuple1
            pdb_list = []
            self.eigenvector_sorted[idx+1] = group
            for graph in group:
                pdb_list.append(graph.graph['pdb_id'])
                # ID is PDB_ID(group number)-index IN PDB e.g. 1U3D(1)-2 is the second subgraph from 1u3d which belongs to the first group
                graph.graph['id'] = graph.graph['pdb_id']+"("+str(idx+1)+")-"+ str(pdb_list.count(graph.graph['pdb_id']))
                graph.graph['group_val'] = key 
                self.specific_subgraphs[graph.graph['id']] = graph


class PDBGroup():
    '''
    Contains all information regarding the group of proteins being analyzed, and all of the of
    the frequent subgraphs identified by the gSpan algorithm.

    Attributes
    ----------
    title: str
        Title of PDB group 
    emaps: dict of str: :class:`~pyemap.emap`
        Dict of PDBs being analyzed by PyeMap. The keys are PDB IDs, meaning that only one :class:`~pyemap.emap` object per PDB ID is allowed.
    temp_dir: str
        Path where temporary files are to be stored   
    frequent_subgraphs: dict of str: :class:`~pyemap.common_paths.FrequentSubgraph`
        Dict of frequent subgraphs found by GSpan. Keys are the unique IDs of the :class:`~pyemap.common_paths.FrequentSubgraph` objects.

    '''
    def __init__(self, title, temp_dir="."):
        ''' Initializes PDBGroup object
        
        Parameters
        ----------
        title: str
            Title of PDB group 
        temp_dir: str
            Path where temporary files are to be stored 
        '''
        self.title = title
        self.emaps = OrderedDict()
        self.temp_dir = temp_dir
        self.frequent_subgraphs = {}
        self.res_to_num_label = {}
        self.num_label_to_res = {}
        self.edge_thresholds = []
        self.parameters = {}
        self.included_eta_moieties = {}
        self.included_chains = {}
        self.included_standard_residues = []
        self.sequences = {}
        self.aligned_sequences = {}

    def _clean_subgraphs(self):
        self.frequent_subgraphs = {}
        self.res_to_num_label = {}
        self.num_label_to_res = {}
        self.edge_thresholds = []
        self.sep_buried_exposed = False

    def _reset_process(self):
        self.parameters = {}
        self.included_eta_moieties = {}
        self.included_chains = {}
        self.included_standard_residues = []
        self.sequences = {}
        self.aligned_sequences = {}
        self._clean_subgraphs()
        
    def _align_sequences(self,chains):
        records = []
        valid_ids = []
        for pdb_id,chain_list in chains.items():
            for chain in chain_list:
                valid_ids.append(pdb_id+":"+chain)
        for pdb_id,emap in self.emaps.items():
            seqIO = SeqIO.parse(emap.file_path, 'pdb-atom')
            for record in seqIO:
                if ":" not in record.id: 
                    record.id = pdb_id + ":" + record.id
                if record.id in valid_ids:
                    seqrec = SeqRecord(record.seq)
                    seqrec.id = record.id
                    seqrec.description = record.id
                    self.sequences[record.id] = record.seq
                    records.append(seqrec)
        SeqIO.write(records, self.temp_dir+"/data.fasta", "fasta")
        inp = self.temp_dir+"/data.fasta"
        out =  self.temp_dir+"/data_aligned.fasta"
        log = self.temp_dir + "/log.txt"
        muscle_cline = MuscleCommandline(input=inp,
                                 out=out,
                                 log=log)
        muscle_cline()
        seqIO = SeqIO.parse(out,"fasta")
        for record in seqIO:
            self.aligned_sequences[record.id] = record.seq
        # now lets save the updated sequence numbers
        for pdb_id,emap in self.emaps.items():
            model = emap.structure[0]
            for chain in model:
                if pdb_id+":"+chain.id in self.aligned_sequences:
                    seq_map={}
                    aligned_seq = self.aligned_sequences[pdb_id+":"+chain.id]
                    original_seq = self.sequences[pdb_id+":"+chain.id]
                    original_idx = 0
                    aligned_idx = 0
                    num_gaps = 0
                    for res in aligned_seq:
                        if not res=="-":
                            residue_obj = list(chain.get_residues())[original_idx]
                            seq_map[residue_obj.id[1]] = aligned_idx
                            original_idx+=1
                        else:
                            num_gaps+=1
                        aligned_idx+=1
                    for res in chain:
                        if res.id[1] in seq_map:
                            res.aligned_residue_number = seq_map[res.id[1]]
                    for resname,res in emap.eta_moieties.items():
                        if res.get_full_id()[2] == chain.id:
                            res.aligned_residue_number = res.id[1] + num_gaps

        


    # chains and eta_moieties should be dictionaries
    def process_emaps(self, chains=None, eta_moieties=None, include_residues=["TYR", "TRP"], **kwargs):
        ''' Processes :class:`~pyemap.emap` objects in order to generate protein graphs. 
        
        For a list of accepted kwargs, see the documentation for :func:`~pyemap.process_data.process`.

        Parameters
        -----------
        chains: dict of str: list of str, optional
            Chains to include for each PDB
        eta_moieties: dict of str: list of str, optional
            Dict containing list of ETA moieties(specified by their residue label) to include for each PDB
        
        Examples
        --------
        >>> my_pg = pyemap.common_paths.PDBGroup()
        >>> # Add pdbs 1u3d,1u3c,6PU0,4I6G,2J4D ...
        >>> eta_moieties = {'1u3d': ['FAD510(A)-2'], '1u3c': ['FAD510(A)-2'], '6PU0': ['FAD501(A)-2'], '4I6G': ['FAD900(A)-2'], '2J4D': ['FAD1498(A)-2']}
        >>> chains = {'1u3d': ['A'], '1u3c': ['A'], '6PU0': ['A'], '4I6G': ['A'], '2J4D': ['A']}
        >>> my_pg.process_emaps(chains=chains,eta_moieties=eta_moieties)

        '''
        # do nothing if processing params are the same
        if kwargs == self.parameters \
            and self.included_chains == chains \
            and self.included_eta_moieties==eta_moieties \
            and self.included_standard_residues == include_residues:
            return
        self._reset_process()
        if chains==None:
            chains = {}
            for pdb_id in self.emaps:
                chains[pdb_id] = [self.emaps[pdb_id].chains[0]]
        self._align_sequences(chains)
        for pdb_id in self.emaps:
            if not eta_moieties==None:
                cur_eta_moieties = eta_moieties[pdb_id]
            else:
                cur_eta_moieties = None
                print("Processing:" + str(pdb_id))
            process(self.emaps[pdb_id], chains=chains[pdb_id], eta_moieties=cur_eta_moieties, include_residues=include_residues, **kwargs)
        self.parameters = kwargs
        self.included_chains = chains
        self.included_eta_moieties = eta_moieties
        self.included_standard_residues = include_residues

    def report_header(self):
        full_str=""
        full_str+= "Generated:\n" + str(datetime.datetime.now()) + "\n"
        full_str+="Parameters:\n"
        if not self.parameters:
            full_str+="Custom.\n"
        else:
            full_str+=str(self.parameters)
            full_str+="\n"
        full_str+="Chains:\n"
        if not self.included_chains:
            full_str+="Custom.\n"
        else:
            full_str+=str(self.included_chains)
            full_str+="\n"
        full_str+="Included non protein moieties:\n"
        if not self.included_eta_moieties:
            full_str+="Custom.\n"
        else:
            full_str+=str(self.included_eta_moieties)
            full_str+="\n"
        full_str+="Edge thresholds:\n"+str(self.edge_thresholds)+"\n"
        full_str+="Node labels:\n"+str(self.res_to_num_label) + "\n"
        full_str+="Node categories:\n" + str(self.num_label_to_res) + "\n"
        return full_str
    
    def general_report(self,dest=None):
        ''' Generates general report of all frequent subgraphs found in the analysis.

        Returns
        -------
        report: str
            General report of all frequent subgraphs found in the analysis.
        '''
        full_str="Overview of all subgraphs:\n"
        full_str+=self.report_header()
        full_str+="\nSubgraphs found:\n\n"
        for fsg in self.frequent_subgraphs:
            full_str+=self.frequent_subgraphs[fsg].general_report()+"\n"
        if dest:
            fi = open(dest, "w")
            fi.write(full_str)
            fi.close()
        return full_str

    def subgraph_report(self,sg_id,dest=None):
        ''' Generates detailed report for a given frequent subgraph.

        Parameters
        -----------
        sg_id: str
            ID corresponding to a :class:`~pyemap.common_paths.FrequentSubgraph` object 
        dest: str, optional
            Destination to write report to file

        Returns
        -------
        report: str
            Detailed report for a particular frequent subgraph.
        '''
        sg = self.frequent_subgraphs[sg_id]
        full_str="Full report for subgraph:" + str(sg_id) + "\n"
        full_str+=self.report_header() + "\n"
        full_str+=sg.full_report()
        if dest:
            fi = open(dest, "w")
            fi.write(full_str)
            fi.close()
        return full_str

    def _set_edge_labels(self):
        all_edge_weights = []
        for id,emap in self.emaps.items():
            G = emap.init_graph
            for edge in G.edges:
                all_edge_weights.append(G.edges[edge]['weight'])
        mean = np.mean(all_edge_weights)
        std_dev = np.std(all_edge_weights)
        self.edge_thresholds = [mean+std_dev,mean+3*std_dev]
        print(self.edge_thresholds)

    def _set_node_labels(self, node_labels, categories):
        self.res_to_num_label={}
        self.num_label_to_res={}
        if categories is not None and ("X" in node_labels or "X" in categories.values()):
            raise KeyError("X is reserved for unknown residue type. Do not use X as a key.")
        if not node_labels:
            num_label = 2
            for res in self.included_standard_residues:
                self.res_to_num_label[res_name_to_char[res]] = num_label
                self.num_label_to_res[num_label] = res_name_to_char[res]
                num_label+=1
            self.num_label_to_res[num_label] = "X"
            self.res_to_num_label["X"] = num_label
        else:
            self.res_to_num_label = node_labels
            self.num_label_to_res = categories
            num_label = len(self.num_label_to_res) + 2
            self.num_label_to_res[num_label] = "X"
            self.res_to_num_label["X"] = num_label
        if self.sep_buried_exposed:
            num_label_to_res = self.num_label_to_res.copy()
            num_categories = len(self.num_label_to_res)
            for num_label,res_label in num_label_to_res.items():
                self.num_label_to_res[num_label+num_categories] = res_label+"_exp"
                self.res_to_num_label[res_label+"_exp"] = num_label+num_categories

    def _get_edge_label(self, G, edge):
        dist = G.edges[edge]['distance']
        label = 2
        for thresh in self.edge_thresholds:
            if dist < thresh:
                return label
            else:
                label += 1
        return label

    # _exp denotes surface exposed residue
    def _get_numerical_node_label(self, u, pdb_id):
        if strip_res_number(u) in char_to_res_name:
            res_name = strip_res_number(u)
            if self.sep_buried_exposed and self.emaps[pdb_id].init_graph.nodes[u]['shape'] == 'box':
                res_name+="_exp"
            result = self.res_to_num_label[res_name]
        elif (pdb_id+"_"+str(u)) in self.res_to_num_label:
            res_label = pdb_id+"_"+str(u)
            if self.sep_buried_exposed and self.emaps[pdb_id].init_graph.nodes[u]['shape'] == 'box':
                res_label+="_exp"
            result = self.res_to_num_label[res_label]
        else:
            if self.sep_buried_exposed and self.emaps[pdb_id].init_graph.nodes[u]['shape'] == 'box':
                result = self.res_to_num_label["X_exp"]
            else:
                result = self.res_to_num_label["X"]
        return result

    def add_emap(self, emap_obj):
        ''' Adds a parsed :class:`~pyemap.emap` object to the PDB group.

        Parameters
        ----------
            emap_obj: :class:`~pyemap.emap` object 
                Parsed PDB generated by :func:`~pyemap.parser.parse` or :func:`~pyemap.parser.fetch_and_parse`
        '''
        if emap_obj.pdb_id not in self.emaps:
            self.emaps[emap_obj.pdb_id] = emap_obj
            print("Added emap object with PDB ID: " + emap_obj.pdb_id)
        else:
            print("An emap object with PDB ID:" + str(emap_obj.pdb_id) + " is already in the data set. Skipping...")

    
    def generate_graph_database(self, node_labels=None, categories=None, sep_buried_exposed=False):
        ''' Generates graph database for analysis by GSpan using specified node labels, node categories, and edge thresholds.

        Parameters
        ----------
            node_labels: dict of str:int, optional
                Dict which maps residue labels to their numerical label for usage in the gSpan algorithm. Labels for non-standard
                residues should be preceded with the 4 character PDB ID followed by an underscore (e.g. 1u3d_FAD510(A)-2)
            categories: dict of int:str, optional
                Dict which maps numerical label to node category (which will appear in generic representation of frequent subgraph)
        
        Examples
        ---------
        >>> # labels for each type of residue included in analysis, including eta moieties grouped as a category
        >>> node_labels = {'1u3d_FAD510(A)-2': 2, '1u3c_FAD510(A)-2': 2, '6PU0_FAD501(A)-2': 2, 
                           '4I6G_FAD900(A)-2': 2, '2J4D_FAD1498(A)-2': 2, '1u3d_ANP511(A)': 3, 
                           'W': 4, 'Y': 5}
        >>> categories = {2: 'Fla', 3: 'Ade', 4: 'W', 5: 'Y'}
        >>> # edge length thresholds for categorizing edges
        >>> my_pg.generate_graph_database(node_labels=node_labels,categories=categories)

        '''
        self._clean_subgraphs()
        self.sep_buried_exposed = sep_buried_exposed
        self._set_node_labels(node_labels, categories)
        self._set_edge_labels()
        f = open(os.path.join(self.temp_dir, 'graphdatabase.txt'), "w")
        for i, key in enumerate(self.emaps):
            G = self.emaps[key].init_graph
            f.write("t # " + str(i) + "\n")
            for i, node in enumerate(G.nodes):
                f.write("v " + str(i) + " " + str(self._get_numerical_node_label(node,key)) + "\n")
            for i, edge in enumerate(G.edges):
                f.write("e " + str(list(G.nodes()).index(edge[0])) + " " + str(list(G.nodes()).index(edge[1])) + " " +
                        str(self._get_edge_label(G, edge)) + "\n")
        f.write("t # -1")
        f.close()

    def run_gspan(self, support, lower_bound=4):
        ''' Runs gSpan algorithm to mine for frequent subgraphs, and then identifies each occurence of each frequent subgraph in each PDB which supports it.
        
        Parameters
        ----------
            support: int, optional
                Minimum support number of subgraphs in the search space 
            lower_bound: int, optional
                Minimum number of nodes for subgraphs in the search space
        '''
        import sys
        old_stdout = sys.stdout
        f = open(os.path.join(self.temp_dir, 'gspan_results.out'), "w")
        sys.stdout = f
        from gspan_mining.config import parser
        from gspan_mining.main import main
        args_str = '-s ' + str(support) + ' -d False -l ' + str(lower_bound) + ' -p False -w True ' + str(
            os.path.join(self.temp_dir, 'graphdatabase.txt'))
        FLAGS, _ = parser.parse_known_args(args=args_str.split())
        gs = main(FLAGS)
        # give us our old standard output back
        sys.stdout = old_stdout
        f.close()
        self._generate_frequent_subgraphs()


    def _generate_frequent_subgraphs(self):
        buff = open(os.path.join(self.temp_dir, 'gspan_results.out'), "r")
        subgraphs = []
        lines = buff.readlines()
        line_idx = 0
        while line_idx < len(lines):
            line = lines[line_idx]
            if len(line.split()) == 3 and line.split()[0] == "t" and line.split()[1] == "#":
                graph_number = line.split()[2]
                line_idx += 1
                start_idx = line_idx - 1
                G = nx.Graph()
                line = lines[line_idx]
                while "---" not in line:
                    line = lines[line_idx]
                    if len(line.split()) > 1 and line.split()[0] == "v":
                        node_idx = int(line.split()[1])
                        node_label = int(line.split()[2])
                        G.add_node(node_idx)
                        G.nodes[node_idx]['label'] = self.num_label_to_res[node_label]
                        G.nodes[node_idx]['num_label'] = node_label
                    if len(line.split()) > 1 and line.split()[0] == "e":
                        idx1 = int(line.split()[1])
                        idx2 = int(line.split()[2])
                        edge_label = int(line.split()[3])
                        G.add_edge(idx1, idx2, label=edge_label)
                        G.edges[(idx1, idx2)]['num_label'] = edge_label
                    if "where" in line:
                        pdb_list_by_index = line[7:-2].strip('][').split(', ')
                        pdb_list_by_index = list(np.array(pdb_list_by_index, dtype=int))
                        pdb_list = list(self.emaps.keys())
                        support = []
                        for idx in pdb_list_by_index:
                            support.append(pdb_list[idx])
                        subgraphs.append(FrequentSubgraph(G, graph_number, support))
                    line_idx += 1
            line_idx += 1
        buff.close()
        subgraphs.sort(key=lambda x: x.support_number, reverse=True)
        for sg in subgraphs:
            specific_subgraphs = []
            for pdb_id in sg.support:
                specific_subgraphs+=self._find_subgraph_in_pdb(sg.generic_subgraph, pdb_id)
            sg.clustering(specific_subgraphs)
            self.frequent_subgraphs[sg.id] = sg


    def _generate_specific_subgraph(self, mapping, protein_graph, generic_subgraph):
        mapping = dict((v, k) for k, v in mapping.items())
        specific_subgraph = generic_subgraph.copy()
        specific_subgraph = nx.relabel_nodes(specific_subgraph, mapping)
        nodes = []
        for node in specific_subgraph.nodes():
            specific_subgraph.nodes[node]['shape'] = protein_graph.nodes[node]['shape']
            specific_subgraph.nodes[node]['label'] = str(node)
            specific_subgraph.nodes[node]['resnum'] = protein_graph.nodes[node]['resnum']
            specific_subgraph.nodes[node]['aligned_resnum'] = protein_graph.nodes[node]['aligned_resnum']
            nodes.append((node,specific_subgraph.nodes[node]['resnum']))
        for edge in specific_subgraph.edges():
            for key in protein_graph.edges[edge]:
                specific_subgraph.edges[edge][key] = protein_graph.edges[edge][key]
        # now the sorted version
        nodes.sort(key=lambda x: x[1])
        nodes_only = []
        for node,resnum in nodes:
            nodes_only.append(node)
        sorted_G = nx.Graph()
        sorted_G.add_nodes_from(nodes_only)
        sorted_G.add_edges_from(specific_subgraph.edges(data=True))
        for node in sorted_G.nodes:
            sorted_G.nodes[node]['shape'] = protein_graph.nodes[node]['shape']
            sorted_G.nodes[node]['label'] = str(node)
            sorted_G.nodes[node]['resnum'] = protein_graph.nodes[node]['resnum']
            sorted_G.graph['pdb_id'] = protein_graph.graph['pdb_id']
            sorted_G.nodes[node]['aligned_resnum'] = protein_graph.nodes[node]['aligned_resnum']
        return sorted_G

    def _find_subgraph_in_pdb(self, generic_subgraph, pdb_id):
        protein_graph = self.emaps[pdb_id].init_graph
        for node in protein_graph.nodes:
            protein_graph.nodes[node]['num_label'] = self._get_numerical_node_label(node,pdb_id)
        for edge in protein_graph.edges:
            protein_graph.edges[edge]['num_label'] = self._get_edge_label(protein_graph, edge)
        GM = isomorphism.GraphMatcher(protein_graph, generic_subgraph, node_match=node_match, edge_match=edge_match)
        subgraph_isos = GM.subgraph_monomorphisms_iter()
        sgs = []
        for mapping in subgraph_isos:
            sg = self._generate_specific_subgraph(mapping, protein_graph, generic_subgraph)
            sgs.append(sg)
        return sgs

    def find_subgraph(self,graph_specification):
        G = nx.Graph()
        prev = None
        for node_idx,node in enumerate(list(graph_specification)):
            G.add_node(node_idx)
            G.nodes[node_idx]['label'] = node 
            G.nodes[node_idx]['num_label'] = self.res_to_num_label[node]
            if node_idx>0:
                G.add_edge(node_idx-1,node_idx,label=2,num_label=2)
        specific_subgraphs = []
        support = []
        for pdb_id in self.emaps:
            specific_subgraphs_for_pdb=self._find_subgraph_in_pdb(G, pdb_id)
            if len(specific_subgraphs_for_pdb)>0:
                specific_subgraphs+=specific_subgraphs_for_pdb
                support.append(pdb_id)
        fs = FrequentSubgraph(G, 1, support)
        fs.clustering(specific_subgraphs)
        self.frequent_subgraphs[fs.id] = fs

    def subgraphs_rmsd(self,subgraph_id,sg1,sg2):
        my_sg = self.frequent_subgraphs[subgraph_id]
        sg1 = my_sg.specific_subgraphs[sg1]
        sg2 = my_sg.specific_subgraphs[sg2]
        emap1 = self.emaps[sg1.graph['pdb_id']]
        emap2 = self.emaps[sg2.graph['pdb_id']]
        atoms1 = []
        atoms2 = []
        for node in sg1.nodes:
            res = emap1.residues[node]
            for atm in res.get_atoms():
                if 'CA' in atm.id:
                    atoms1.append(atm)
        for node in sg2.nodes:
            res = emap2.residues[node]
            for atm in res.get_atoms():
                if 'CA' in atm.id:
                    atoms2.append(atm)
        si = Superimposer()
        si.set_atoms(atoms1, atoms2)
        return si.rms
        #print(f"RMSD between structures: {si.rms:4.2f}")







